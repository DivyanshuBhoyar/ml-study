{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/divyan-18871/Developer/Learning/ml_/pyenv/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "! ct2-transformers-converter --model Helsinki-NLP/opus-mt-en-hi --output_dir opus-mt-en-hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divyan-18871/Developer/Learning/ml_/pyenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-19 10:56:52.315655: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-19 10:56:52.317162: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-19 10:56:52.356943: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-19 10:56:52.999613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नमस्कार दुनिया!\n"
     ]
    }
   ],
   "source": [
    "import ctranslate2\n",
    "import transformers\n",
    "\n",
    "translator = ctranslate2.Translator(\"opus-mt-en-hi\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "\n",
    "source = tokenizer.convert_ids_to_tokens(tokenizer.encode(\"Hello world!\"))\n",
    "results = translator.translate_batch([source])\n",
    "target = results[0].hypotheses[0]\n",
    "\n",
    "print(tokenizer.decode(tokenizer.convert_tokens_to_ids(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "वह रात के खाने के लिए भूखा है\n"
     ]
    }
   ],
   "source": [
    "source = tokenizer.convert_ids_to_tokens(tokenizer.encode(\"She is hungry for dinner\"))\n",
    "\n",
    "results = translator.translate_batch([source], beam_size=3)\n",
    "target = results[0].hypotheses[0]\n",
    "\n",
    "print(tokenizer.decode(tokenizer.convert_tokens_to_ids(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "उसने कभी पीछे नहीं देखा। उसका नाम न केवल देश में जाना गया, बल्कि विदेश भी गया। उसने अपनी बोली पर भी टिप्पणी की, और कहा कि उसके मन में कोई दूसरा विचार नहीं है। नयी चुनौतियों का सामना करना पड़ेगा।\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"He never looked back.\"\n",
    "    \"His name became known not only in the country, but also abroad.\"\n",
    "    \"He also commented on his speech, saying that he didn't have any second thoughts in his mind.\"\n",
    "    \"New challenges will be faced strongly.\"\n",
    ")\n",
    "\n",
    "source = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "results = translator.translate_batch([source])\n",
    "print(len(results[0].hypotheses))\n",
    "target = results[0].hypotheses[0]\n",
    "\n",
    "print(tokenizer.decode(tokenizer.convert_tokens_to_ids(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search: मौसम आज सुंदर है.\n",
      "Random sampling:\n",
      "मौसम आज बहुत सुंदर है.\n",
      "मौसम आज बहुत खूबसूरत है.\n",
      "आज मौसम खूबसूरत है.\n",
      "\n",
      "\n",
      "Beam search: मैं कोड सीखने जा रहा हूँ.\n",
      "Random sampling:\n",
      "मैं कोड करने के लिए सीख रहा हूँ.\n",
      "मैं कोड सीखने जा रहा हूँ।\n",
      "मैं कोड सीख रहा हूँ.\n",
      "\n",
      "\n",
      "Beam search: वह एक महान शिक्षक है.\n",
      "Random sampling:\n",
      "वह एक बढ़िया शिक्षक है.\n",
      "वह एक काबिल शिक्षक है।\n",
      "वो एक टीचर है।\n",
      "\n",
      "\n",
      "Beam search: हम पहाड़ों के लिए एक यात्रा योजना बना रहे हैं.\n",
      "Random sampling:\n",
      "हम पहाड़ों के लिए एक यात्रा योजना बना रहे हैं।\n",
      "हम पहाड़ों की यात्रा की योजना बना रहे हैं।\n",
      "हम पहाड़ों की एक यात्रा की योजना बना रहे.\n",
      "\n",
      "\n",
      "Beam search: यह पुस्तक बहुत दिलचस्प है.\n",
      "Random sampling:\n",
      "यह पुस्तक बहुत दिलचस्प है.\n",
      "यह किताब बहुत दिलचस्प है.\n",
      "यह पुस्तक बहुत दिलचस्प है।\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"I am learning to code.\",\n",
    "    \"She is an excellent teacher.\",\n",
    "    \"We are planning a trip to the mountains.\",\n",
    "    \"This book is very interesting.\"\n",
    "]\n",
    "model = translator\n",
    "for sentence in sentences:\n",
    "    source = tokenizer.convert_ids_to_tokens(tokenizer.encode(sentence))\n",
    "\n",
    "    # Beam search\n",
    "    results = model.translate_batch([source], beam_size=4)\n",
    "    target = results[0].hypotheses[0]\n",
    "    print(\"Beam search:\", tokenizer.decode(tokenizer.convert_tokens_to_ids(target)))\n",
    "\n",
    "    # Random sampling\n",
    "    results = model.translate_batch([source], beam_size=1, sampling_topk=10, num_hypotheses=3)\n",
    "    print(\"Random sampling:\")\n",
    "    for hypothesis in results[0].hypotheses:\n",
    "        print(tokenizer.decode(tokenizer.convert_tokens_to_ids(hypothesis)))\n",
    "\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../fseq/source_test.txt\", \"r\") as f:\n",
    "    source_sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "for sentence in source_sentences:\n",
    "    source = tokenizer.convert_ids_to_tokens(tokenizer.encode(sentence))\n",
    "    result = translator.translate_batch([source], beam_size=5)\n",
    "    translations.append(result[0].hypotheses[0])\n",
    "\n",
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../fseq/target_test.txt\", \"r\") as f:\n",
    "    reference_translations = [f.readlines()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
