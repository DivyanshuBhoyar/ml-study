{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'and powerful' 'demonstration' 'demonstration purposes'\n",
      " 'fascinating' 'fascinating and' 'for' 'for demonstration' 'is'\n",
      " 'is fascinating' 'is sample' 'it' 'it for' 'nlp' 'nlp is' 'powerful'\n",
      " 'purposes' 'sample' 'sample sentence' 'sentence' 'this' 'this is' 'use'\n",
      " 'use it' 'we' 'we will' 'will' 'will use']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example text data\n",
    "corpus = [\"This is a sample sentence.\",\n",
    "          \"We will use it for demonstration purposes.\",\n",
    "          \"NLP is fascinating and powerful.\"]\n",
    "\n",
    "# Initialize CountVectorizer with ngram_range=(1, 2)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the text data using vectorizer\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (n-grams)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the feature names\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "  -0.57735027  0.57735027 -0.57735027  0.        ]\n",
      " [ 0.          0.         -0.5         0.          0.          0.5\n",
      "   0.          0.5        -0.5         0.        ]\n",
      " [ 0.          0.40824829  0.40824829  0.         -0.40824829 -0.40824829\n",
      "   0.          0.40824829 -0.40824829  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  -0.57735027  0.57735027 -0.57735027  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create the Hashing Vectorizer\n",
    "hashing_vectorizer = HashingVectorizer(n_features=10)  # n_features determines the size of the output vector\n",
    "\n",
    "# Transform the corpus\n",
    "hashing_matrix = hashing_vectorizer.transform(corpus)\n",
    "\n",
    "# The hashing_matrix is a sparse matrix representing the hashed feature values.\n",
    "# If you need a dense representation, you can convert it using toarray():\n",
    "dense_hashing_matrix = hashing_matrix.toarray()\n",
    "print(dense_hashing_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
