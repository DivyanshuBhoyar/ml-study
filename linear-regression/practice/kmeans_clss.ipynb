{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents - 4 categories\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.graphics\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    subset=\"all\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "labels = dataset.target\n",
    "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(f\"{len(dataset.data)} documents - {true_k} categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization done in 0.323 s\n",
      "n_samples: 3387, n_features: 7929\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements assigned to each cluster: [2050  711  180  446]\n",
      "Number of elements assigned to each cluster: [ 575  619  485 1708]\n",
      "Number of elements assigned to each cluster: [   1    1    1 3384]\n",
      "Number of elements assigned to each cluster: [1887  311  332  857]\n",
      "Number of elements assigned to each cluster: [1688  636  454  609]\n",
      "\n",
      "True number of documents in each category according to the class labels: [799 973 987 628]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for seed in range(5):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=true_k,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        random_state=seed,\n",
    "    ).fit(X_tfidf)\n",
    "    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "print()\n",
    "print(\n",
    "    \"True number of documents in each category according to the class labels: \"\n",
    "    f\"{category_sizes}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering done in 0.08 ± 0.01 s \n",
      "Homogeneity: 0.358 ± 0.007\n",
      "Completeness: 0.405 ± 0.005\n",
      "V-measure: 0.380 ± 0.005\n",
      "Adjusted Rand-Index: 0.217 ± 0.011\n",
      "Silhouette Coefficient: 0.007 ± 0.000\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(\n",
    "    n_clusters=true_k,\n",
    "    max_iter=100,\n",
    "    n_init=5,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, X_tfidf, name=\"KMeans\\non tf-idf vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA done in 0.355 s\n",
      "Explained variance of the SVD step: 18.4%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))\n",
    "t0 = time()\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"LSA done in {time() - t0:.3f} s\")\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering done in 0.13 ± 0.01 s \n",
      "Homogeneity: 0.404 ± 0.003\n",
      "Completeness: 0.443 ± 0.014\n",
      "V-measure: 0.422 ± 0.006\n",
      "Adjusted Rand-Index: 0.314 ± 0.012\n",
      "Silhouette Coefficient: 0.030 ± 0.000\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(\n",
    "    n_clusters=true_k,\n",
    "    max_iter=100,\n",
    "    n_init=1,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, X_lsa, name=\"KMeans\\nwith LSA on tf-idf vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: space launch orbit shuttle nasa earth moon like mission satellite \n",
      "Cluster 1: graphics thanks image files file program know looking help format \n",
      "Cluster 2: god people jesus bible believe don say religion think christian \n",
      "Cluster 3: just think don know like time ve does say did \n"
     ]
    }
   ],
   "source": [
    "original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print(f\"Cluster {i}: \", end=\"\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\"{terms[ind]} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization done in 1.997 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "\n",
    "lsa_vectorizer = make_pipeline(\n",
    "    HashingVectorizer(stop_words=\"english\", n_features=50_000),\n",
    "    TfidfTransformer(),\n",
    "    TruncatedSVD(n_components=100, random_state=0),\n",
    "    Normalizer(copy=False),\n",
    ")\n",
    "\n",
    "t0 = time()\n",
    "X_hashed_lsa = lsa_vectorizer.fit_transform(dataset.data)\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering done in 0.13 ± 0.01 s \n",
      "Homogeneity: 0.235 ± 0.137\n",
      "Completeness: 0.321 ± 0.065\n",
      "V-measure: 0.257 ± 0.124\n",
      "Adjusted Rand-Index: 0.183 ± 0.140\n",
      "Silhouette Coefficient: 0.018 ± 0.014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(\n",
    "    n_clusters=true_k,\n",
    "    n_init=1,\n",
    "    init_size=1000,\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(\n",
    "    minibatch_kmeans,\n",
    "    X_lsa,\n",
    "    name=\"MiniBatchKMeans\\nwith LSA on tf-idf vectors\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minibatch_kmeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fit_and_evaluate(\n\u001b[0;32m----> 2\u001b[0m     minibatch_kmeans,\n\u001b[1;32m      3\u001b[0m     X_hashed_lsa,\n\u001b[1;32m      4\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMiniBatchKMeans\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mwith LSA on hashed vectors\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'minibatch_kmeans' is not defined"
     ]
    }
   ],
   "source": [
    "fit_and_evaluate(\n",
    "    minibatch_kmeans,\n",
    "    X_hashed_lsa,\n",
    "    name=\"MiniBatchKMeans\\nwith LSA on hashed vectors\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "df = pd.DataFrame(evaluations[::-1]).set_index(\"estimator\")\n",
    "df_std = pd.DataFrame(evaluations_std[::-1]).set_index(\"estimator\")\n",
    "\n",
    "df.drop(\n",
    "    [\"train_time\"],\n",
    "    axis=\"columns\",\n",
    ").plot.barh(ax=ax0, xerr=df_std)\n",
    "ax0.set_xlabel(\"Clustering scores\")\n",
    "ax0.set_ylabel(\"\")\n",
    "\n",
    "df[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\n",
    "ax1.set_xlabel(\"Clustering time (s)\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
