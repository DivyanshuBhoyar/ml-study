{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST('./data/MNIST_data/', download=True, train=True, transform=transformer)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ClassifierNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ly1 = nn.Linear(784, 128)\n",
    "        self.ly2 = nn.Linear(128,64)\n",
    "        self.ly3 = nn.Linear(64, 10)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1) #flatten\n",
    "\n",
    "        x = F.relu(self.ly1(x))\n",
    "        x = F.relu(self.ly2(x))\n",
    "\n",
    "        return F.log_softmax(self.ly3(x), dim=1)\n",
    "\n",
    "\n",
    "EPOCHS = 70\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, log_title=\"\"):\n",
    "    j_history = []\n",
    "    for e in range(EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for imgs, labels in trainloader:\n",
    "            probab = model(imgs)\n",
    "            loss = criterion(probab, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        if log_title:\n",
    "            print(f'{log_title} @ epoch {e+1} :: loss = {epoch_loss/len(trainloader)}')\n",
    "        j_history.append(epoch_loss/len(trainloader))\n",
    "    \n",
    "    return j_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD @ epoch 1 :: loss = 0.6152523378732362\n",
      "SGD @ epoch 2 :: loss = 0.2807812547243671\n",
      "SGD @ epoch 3 :: loss = 0.21742715007031777\n",
      "SGD @ epoch 4 :: loss = 0.17581133170327398\n",
      "SGD @ epoch 5 :: loss = 0.14681547358910094\n",
      "SGD @ epoch 6 :: loss = 0.12500252016285843\n",
      "SGD @ epoch 7 :: loss = 0.11007254887054534\n",
      "SGD @ epoch 8 :: loss = 0.09772952987220702\n",
      "SGD @ epoch 9 :: loss = 0.08768878578802129\n",
      "SGD @ epoch 10 :: loss = 0.07914540283818806\n",
      "SGD @ epoch 11 :: loss = 0.07235865304464979\n",
      "SGD @ epoch 12 :: loss = 0.06655947653763393\n",
      "SGD @ epoch 13 :: loss = 0.06117905895096629\n",
      "SGD @ epoch 14 :: loss = 0.056332898391748286\n",
      "SGD @ epoch 15 :: loss = 0.05182529879567116\n",
      "SGD @ epoch 16 :: loss = 0.04870602336458004\n",
      "SGD @ epoch 17 :: loss = 0.04445923399229818\n",
      "SGD @ epoch 18 :: loss = 0.041548487062363436\n",
      "SGD @ epoch 19 :: loss = 0.03848372612209089\n",
      "SGD @ epoch 20 :: loss = 0.035908889348012035\n",
      "SGD @ epoch 21 :: loss = 0.03312955668525222\n",
      "SGD @ epoch 22 :: loss = 0.030855967105601602\n",
      "SGD @ epoch 23 :: loss = 0.028434587969642773\n",
      "SGD @ epoch 24 :: loss = 0.026641790532749763\n",
      "SGD @ epoch 25 :: loss = 0.025012725561327026\n",
      "SGD @ epoch 26 :: loss = 0.023221013575557992\n",
      "SGD @ epoch 27 :: loss = 0.02132084247865168\n",
      "SGD @ epoch 28 :: loss = 0.01933989036255635\n",
      "SGD @ epoch 29 :: loss = 0.01826243216980066\n",
      "SGD @ epoch 30 :: loss = 0.01679018012558007\n",
      "SGD @ epoch 31 :: loss = 0.015707187562138895\n",
      "SGD @ epoch 32 :: loss = 0.01448493977960349\n",
      "SGD @ epoch 33 :: loss = 0.013162866792561404\n",
      "SGD @ epoch 34 :: loss = 0.01194173636870632\n",
      "SGD @ epoch 35 :: loss = 0.011776279815885204\n",
      "SGD @ epoch 36 :: loss = 0.010316972485939557\n",
      "SGD @ epoch 37 :: loss = 0.00976258399337828\n",
      "SGD @ epoch 38 :: loss = 0.008860532841300155\n",
      "SGD @ epoch 39 :: loss = 0.00804438832986875\n",
      "SGD @ epoch 40 :: loss = 0.0072699774554727085\n",
      "SGD @ epoch 41 :: loss = 0.006453479939229242\n",
      "SGD @ epoch 42 :: loss = 0.005826878954417758\n",
      "SGD @ epoch 43 :: loss = 0.0053043411907037225\n",
      "SGD @ epoch 44 :: loss = 0.0053105313190225404\n",
      "SGD @ epoch 45 :: loss = 0.004361641485193027\n",
      "SGD @ epoch 46 :: loss = 0.00430786219058335\n",
      "SGD @ epoch 47 :: loss = 0.004087674653838398\n",
      "SGD @ epoch 48 :: loss = 0.0038877338315314327\n",
      "SGD @ epoch 49 :: loss = 0.00330766283625018\n",
      "SGD @ epoch 50 :: loss = 0.0031942820478395658\n",
      "SGD @ epoch 51 :: loss = 0.0028542783452559194\n",
      "SGD @ epoch 52 :: loss = 0.002647965817166101\n",
      "SGD @ epoch 53 :: loss = 0.002428833581780248\n",
      "SGD @ epoch 54 :: loss = 0.0023221140675817498\n",
      "SGD @ epoch 55 :: loss = 0.002516098311780937\n",
      "SGD @ epoch 56 :: loss = 0.0020090469666773133\n",
      "SGD @ epoch 57 :: loss = 0.001941414509630378\n",
      "SGD @ epoch 58 :: loss = 0.0018353233514714173\n",
      "SGD @ epoch 59 :: loss = 0.0017389741246052993\n",
      "SGD @ epoch 60 :: loss = 0.0016825622619481538\n",
      "SGD @ epoch 61 :: loss = 0.0016678877429031867\n",
      "SGD @ epoch 62 :: loss = 0.0015118182887693856\n",
      "SGD @ epoch 63 :: loss = 0.0014487035790737757\n",
      "SGD @ epoch 64 :: loss = 0.0014043616197758112\n",
      "SGD @ epoch 65 :: loss = 0.0013337027864328507\n",
      "SGD @ epoch 66 :: loss = 0.0012862526754204094\n",
      "SGD @ epoch 67 :: loss = 0.0012703716344667226\n",
      "SGD @ epoch 68 :: loss = 0.0011852827617768703\n",
      "SGD @ epoch 69 :: loss = 0.0011578264811178398\n",
      "SGD @ epoch 70 :: loss = 0.001106462015785874\n"
     ]
    }
   ],
   "source": [
    "model = ClassifierNN()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03)\n",
    "sgd_losses = train(model, optimizer, \"SGD\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum @ epoch 1 :: loss = 0.3506138671789247\n",
      "Momentum @ epoch 2 :: loss = 0.1687912673228728\n",
      "Momentum @ epoch 3 :: loss = 0.133046903224515\n",
      "Momentum @ epoch 4 :: loss = 0.11100080851783185\n",
      "Momentum @ epoch 5 :: loss = 0.09949152359216691\n",
      "Momentum @ epoch 6 :: loss = 0.08498761556627574\n",
      "Momentum @ epoch 7 :: loss = 0.07483103296740341\n",
      "Momentum @ epoch 8 :: loss = 0.06869970910284154\n",
      "Momentum @ epoch 9 :: loss = 0.061800605370174115\n",
      "Momentum @ epoch 10 :: loss = 0.05942660960505209\n",
      "Momentum @ epoch 11 :: loss = 0.056431063069978846\n",
      "Momentum @ epoch 12 :: loss = 0.05278043846440524\n",
      "Momentum @ epoch 13 :: loss = 0.05138230281383065\n",
      "Momentum @ epoch 14 :: loss = 0.04863370526810366\n",
      "Momentum @ epoch 15 :: loss = 0.043976276353492015\n",
      "Momentum @ epoch 16 :: loss = 0.04585115477683329\n",
      "Momentum @ epoch 17 :: loss = 0.04103133875273738\n",
      "Momentum @ epoch 18 :: loss = 0.0412435893109211\n",
      "Momentum @ epoch 19 :: loss = 0.03707508980534533\n",
      "Momentum @ epoch 20 :: loss = 0.034315812059623745\n",
      "Momentum @ epoch 21 :: loss = 0.035729959034976534\n",
      "Momentum @ epoch 22 :: loss = 0.03347065308624026\n",
      "Momentum @ epoch 23 :: loss = 0.027412733939567658\n",
      "Momentum @ epoch 24 :: loss = 0.035016027473011684\n",
      "Momentum @ epoch 25 :: loss = 0.03178247903139827\n",
      "Momentum @ epoch 26 :: loss = 0.03238224014679298\n",
      "Momentum @ epoch 27 :: loss = 0.029536117354424133\n",
      "Momentum @ epoch 28 :: loss = 0.030447478758908554\n",
      "Momentum @ epoch 29 :: loss = 0.02979956860371628\n",
      "Momentum @ epoch 30 :: loss = 0.035299072279427524\n",
      "Momentum @ epoch 31 :: loss = 0.034860249649812304\n",
      "Momentum @ epoch 32 :: loss = 0.027107022323210182\n",
      "Momentum @ epoch 33 :: loss = 0.026933939458646347\n",
      "Momentum @ epoch 34 :: loss = 0.02835897105233026\n",
      "Momentum @ epoch 35 :: loss = 0.022850273121397265\n",
      "Momentum @ epoch 36 :: loss = 0.027390953479127022\n",
      "Momentum @ epoch 37 :: loss = 0.026381437033651096\n",
      "Momentum @ epoch 38 :: loss = 0.03429278483499152\n",
      "Momentum @ epoch 39 :: loss = 0.03407368239394089\n",
      "Momentum @ epoch 40 :: loss = 0.026031001450394137\n",
      "Momentum @ epoch 41 :: loss = 0.01788111981812808\n",
      "Momentum @ epoch 42 :: loss = 0.020639150594069917\n",
      "Momentum @ epoch 43 :: loss = 0.029146149302663533\n",
      "Momentum @ epoch 44 :: loss = 0.03164259324703448\n",
      "Momentum @ epoch 45 :: loss = 0.025458925958598572\n",
      "Momentum @ epoch 46 :: loss = 0.023433159673576846\n",
      "Momentum @ epoch 47 :: loss = 0.02315177063818081\n",
      "Momentum @ epoch 48 :: loss = 0.04561482541820307\n",
      "Momentum @ epoch 49 :: loss = 0.02811359004908542\n",
      "Momentum @ epoch 50 :: loss = 0.02952073430666561\n",
      "Momentum @ epoch 51 :: loss = 0.018720805543922445\n",
      "Momentum @ epoch 52 :: loss = 0.012770965982121876\n",
      "Momentum @ epoch 53 :: loss = 0.01817123328501807\n",
      "Momentum @ epoch 54 :: loss = 0.03277650887092345\n",
      "Momentum @ epoch 55 :: loss = 0.04109740383974012\n",
      "Momentum @ epoch 56 :: loss = 0.034775193494694\n",
      "Momentum @ epoch 57 :: loss = 0.023112631657080194\n",
      "Momentum @ epoch 58 :: loss = 0.023846417387852915\n",
      "Momentum @ epoch 59 :: loss = 0.027645558765988114\n",
      "Momentum @ epoch 60 :: loss = 0.0222432629161495\n",
      "Momentum @ epoch 61 :: loss = 0.030861145025063173\n",
      "Momentum @ epoch 62 :: loss = 0.026654520234788558\n",
      "Momentum @ epoch 63 :: loss = 0.029633309396733858\n",
      "Momentum @ epoch 64 :: loss = 0.020607411554263478\n",
      "Momentum @ epoch 65 :: loss = 0.025190259810782677\n",
      "Momentum @ epoch 66 :: loss = 0.02928515485793606\n",
      "Momentum @ epoch 67 :: loss = 0.03115847909692543\n",
      "Momentum @ epoch 68 :: loss = 0.03461129169934813\n",
      "Momentum @ epoch 69 :: loss = 0.04404752943040537\n",
      "Momentum @ epoch 70 :: loss = 0.03388400921148274\n"
     ]
    }
   ],
   "source": [
    "model = ClassifierNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9)\n",
    "mtm_losses = train(model, optimizer, \"Momentum\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nesterov @ epoch 1 :: loss = 0.32700309465760424\n",
      "Nesterov @ epoch 2 :: loss = 0.15528091838134567\n",
      "Nesterov @ epoch 3 :: loss = 0.1229873645588009\n",
      "Nesterov @ epoch 4 :: loss = 0.10092318460751357\n",
      "Nesterov @ epoch 5 :: loss = 0.0880665971608614\n",
      "Nesterov @ epoch 6 :: loss = 0.07713217000996542\n",
      "Nesterov @ epoch 7 :: loss = 0.07247735617899778\n",
      "Nesterov @ epoch 8 :: loss = 0.06359403644430775\n",
      "Nesterov @ epoch 9 :: loss = 0.05971834635522628\n",
      "Nesterov @ epoch 10 :: loss = 0.057002115394122824\n",
      "Nesterov @ epoch 11 :: loss = 0.05106404230393507\n",
      "Nesterov @ epoch 12 :: loss = 0.05221546721423908\n",
      "Nesterov @ epoch 13 :: loss = 0.05182842702497568\n",
      "Nesterov @ epoch 14 :: loss = 0.04466537788274141\n",
      "Nesterov @ epoch 15 :: loss = 0.042764380485774627\n",
      "Nesterov @ epoch 16 :: loss = 0.03856047143492458\n",
      "Nesterov @ epoch 17 :: loss = 0.03688265363974684\n",
      "Nesterov @ epoch 18 :: loss = 0.038468587592150195\n",
      "Nesterov @ epoch 19 :: loss = 0.04187263135745156\n",
      "Nesterov @ epoch 20 :: loss = 0.04083148409586538\n",
      "Nesterov @ epoch 21 :: loss = 0.03710169372796077\n",
      "Nesterov @ epoch 22 :: loss = 0.035294675115236034\n",
      "Nesterov @ epoch 23 :: loss = 0.030638996198234313\n",
      "Nesterov @ epoch 24 :: loss = 0.035917322644549485\n",
      "Nesterov @ epoch 25 :: loss = 0.03024238707006868\n",
      "Nesterov @ epoch 26 :: loss = 0.02872326706513906\n",
      "Nesterov @ epoch 27 :: loss = 0.028016095272453124\n",
      "Nesterov @ epoch 28 :: loss = 0.03233788477779169\n",
      "Nesterov @ epoch 29 :: loss = 0.028637903979048085\n",
      "Nesterov @ epoch 30 :: loss = 0.03410029013004362\n",
      "Nesterov @ epoch 31 :: loss = 0.029146289261183993\n",
      "Nesterov @ epoch 32 :: loss = 0.03202651126357043\n",
      "Nesterov @ epoch 33 :: loss = 0.02870033414948925\n",
      "Nesterov @ epoch 34 :: loss = 0.026946034086819223\n",
      "Nesterov @ epoch 35 :: loss = 0.024581344346067215\n",
      "Nesterov @ epoch 36 :: loss = 0.034419260962946174\n",
      "Nesterov @ epoch 37 :: loss = 0.02734539899434581\n",
      "Nesterov @ epoch 38 :: loss = 0.029273373684710377\n",
      "Nesterov @ epoch 39 :: loss = 0.02689731972965976\n",
      "Nesterov @ epoch 40 :: loss = 0.022536955447271394\n",
      "Nesterov @ epoch 41 :: loss = 0.022714595656692708\n",
      "Nesterov @ epoch 42 :: loss = 0.028021818592558\n",
      "Nesterov @ epoch 43 :: loss = 0.022523722169711366\n",
      "Nesterov @ epoch 44 :: loss = 0.025906415292485482\n",
      "Nesterov @ epoch 45 :: loss = 0.02380212065815131\n",
      "Nesterov @ epoch 46 :: loss = 0.026246333514591894\n",
      "Nesterov @ epoch 47 :: loss = 0.026340507063620726\n",
      "Nesterov @ epoch 48 :: loss = 0.02250349574053643\n",
      "Nesterov @ epoch 49 :: loss = 0.024196376305945434\n",
      "Nesterov @ epoch 50 :: loss = 0.026342130226785235\n",
      "Nesterov @ epoch 51 :: loss = 0.028540708523441788\n",
      "Nesterov @ epoch 52 :: loss = 0.04310145935491487\n",
      "Nesterov @ epoch 53 :: loss = 0.027352818982512666\n",
      "Nesterov @ epoch 54 :: loss = 0.02698022825035147\n",
      "Nesterov @ epoch 55 :: loss = 0.029431418016367877\n",
      "Nesterov @ epoch 56 :: loss = 0.0320091101752701\n",
      "Nesterov @ epoch 57 :: loss = 0.029508599150629495\n",
      "Nesterov @ epoch 58 :: loss = 0.0232350240255935\n",
      "Nesterov @ epoch 59 :: loss = 0.023876340614768038\n",
      "Nesterov @ epoch 60 :: loss = 0.022205672767838035\n",
      "Nesterov @ epoch 61 :: loss = 0.028351114862095933\n",
      "Nesterov @ epoch 62 :: loss = 0.03164125297386161\n",
      "Nesterov @ epoch 63 :: loss = 0.03578470069679901\n",
      "Nesterov @ epoch 64 :: loss = 0.03121142365661674\n",
      "Nesterov @ epoch 65 :: loss = 0.029672483649274016\n",
      "Nesterov @ epoch 66 :: loss = 0.03061517814913692\n",
      "Nesterov @ epoch 67 :: loss = 0.026768704840839576\n",
      "Nesterov @ epoch 68 :: loss = 0.04330469438359389\n",
      "Nesterov @ epoch 69 :: loss = 0.03410192076247521\n",
      "Nesterov @ epoch 70 :: loss = 0.03772234205964474\n"
     ]
    }
   ],
   "source": [
    "model = ClassifierNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9, nesterov=True)\n",
    "nestv_losses = train(model, optimizer, \"Nesterov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adagrad @ epoch 1 :: loss = 0.39952713315849747\n",
      "Adagrad @ epoch 2 :: loss = 0.16495432685226646\n",
      "Adagrad @ epoch 3 :: loss = 0.128333741848641\n",
      "Adagrad @ epoch 4 :: loss = 0.10873087396376582\n",
      "Adagrad @ epoch 5 :: loss = 0.09648636189837462\n",
      "Adagrad @ epoch 6 :: loss = 0.08684126697758686\n",
      "Adagrad @ epoch 7 :: loss = 0.0788474696328931\n",
      "Adagrad @ epoch 8 :: loss = 0.07305026507831172\n",
      "Adagrad @ epoch 9 :: loss = 0.06780134544753508\n",
      "Adagrad @ epoch 10 :: loss = 0.06358407477864118\n",
      "Adagrad @ epoch 11 :: loss = 0.058914115763892896\n",
      "Adagrad @ epoch 12 :: loss = 0.0551839428606318\n",
      "Adagrad @ epoch 13 :: loss = 0.05258623373072678\n",
      "Adagrad @ epoch 14 :: loss = 0.048664606813498254\n",
      "Adagrad @ epoch 15 :: loss = 0.04621072389733499\n",
      "Adagrad @ epoch 16 :: loss = 0.04377545394325581\n",
      "Adagrad @ epoch 17 :: loss = 0.0419877212676309\n",
      "Adagrad @ epoch 18 :: loss = 0.039124745799703646\n",
      "Adagrad @ epoch 19 :: loss = 0.03718797534407337\n",
      "Adagrad @ epoch 20 :: loss = 0.03568633419383607\n",
      "Adagrad @ epoch 21 :: loss = 0.03437631261517117\n",
      "Adagrad @ epoch 22 :: loss = 0.03239714786924644\n",
      "Adagrad @ epoch 23 :: loss = 0.03082271687165677\n",
      "Adagrad @ epoch 24 :: loss = 0.029361082693941193\n",
      "Adagrad @ epoch 25 :: loss = 0.028066083648379075\n",
      "Adagrad @ epoch 26 :: loss = 0.026321523768483783\n",
      "Adagrad @ epoch 27 :: loss = 0.025497680565199334\n",
      "Adagrad @ epoch 28 :: loss = 0.024424572865022465\n",
      "Adagrad @ epoch 29 :: loss = 0.023387622437377804\n",
      "Adagrad @ epoch 30 :: loss = 0.02202294570337266\n",
      "Adagrad @ epoch 31 :: loss = 0.021508735634556818\n",
      "Adagrad @ epoch 32 :: loss = 0.020436452962629404\n",
      "Adagrad @ epoch 33 :: loss = 0.019454509190627252\n",
      "Adagrad @ epoch 34 :: loss = 0.018827960092096783\n",
      "Adagrad @ epoch 35 :: loss = 0.018138944208415955\n",
      "Adagrad @ epoch 36 :: loss = 0.017239731755842373\n",
      "Adagrad @ epoch 37 :: loss = 0.016481276821724372\n",
      "Adagrad @ epoch 38 :: loss = 0.015942215569566057\n",
      "Adagrad @ epoch 39 :: loss = 0.015242902212000555\n",
      "Adagrad @ epoch 40 :: loss = 0.014607374541066052\n",
      "Adagrad @ epoch 41 :: loss = 0.01406744849703797\n",
      "Adagrad @ epoch 42 :: loss = 0.013545890049666349\n",
      "Adagrad @ epoch 43 :: loss = 0.012746251852797427\n",
      "Adagrad @ epoch 44 :: loss = 0.012251530975430533\n",
      "Adagrad @ epoch 45 :: loss = 0.01180298339683443\n",
      "Adagrad @ epoch 46 :: loss = 0.011546538317178154\n",
      "Adagrad @ epoch 47 :: loss = 0.010999440632367468\n",
      "Adagrad @ epoch 48 :: loss = 0.010705773129007782\n",
      "Adagrad @ epoch 49 :: loss = 0.010312303839625466\n",
      "Adagrad @ epoch 50 :: loss = 0.009866457010999368\n",
      "Adagrad @ epoch 51 :: loss = 0.009455495450741497\n",
      "Adagrad @ epoch 52 :: loss = 0.009131694293429162\n",
      "Adagrad @ epoch 53 :: loss = 0.008794697986343933\n",
      "Adagrad @ epoch 54 :: loss = 0.008445768622244134\n",
      "Adagrad @ epoch 55 :: loss = 0.00822742893736609\n",
      "Adagrad @ epoch 56 :: loss = 0.007704701153052\n",
      "Adagrad @ epoch 57 :: loss = 0.0074773089941476845\n",
      "Adagrad @ epoch 58 :: loss = 0.0072877896300676875\n",
      "Adagrad @ epoch 59 :: loss = 0.007072838795208025\n",
      "Adagrad @ epoch 60 :: loss = 0.006624350792882013\n",
      "Adagrad @ epoch 61 :: loss = 0.006476561288762121\n",
      "Adagrad @ epoch 62 :: loss = 0.0064322495191886245\n",
      "Adagrad @ epoch 63 :: loss = 0.006094773923474943\n",
      "Adagrad @ epoch 64 :: loss = 0.005966413307217394\n",
      "Adagrad @ epoch 65 :: loss = 0.005691918689749953\n",
      "Adagrad @ epoch 66 :: loss = 0.005506562564382032\n",
      "Adagrad @ epoch 67 :: loss = 0.005327542026477955\n",
      "Adagrad @ epoch 68 :: loss = 0.005196891239792633\n",
      "Adagrad @ epoch 69 :: loss = 0.005038464715822102\n",
      "Adagrad @ epoch 70 :: loss = 0.004894142557105549\n"
     ]
    }
   ],
   "source": [
    "model = ClassifierNN()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.03, eps=1e-8 )\n",
    "adgd_losses = train(model, optimizer, \"Adagrad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSProp @ epoch 1 :: loss = 13.951126111977135\n",
      "RMSProp @ epoch 2 :: loss = 2.2638618860925948\n",
      "RMSProp @ epoch 3 :: loss = 2.304280276491698\n",
      "RMSProp @ epoch 4 :: loss = 2.304263625317799\n",
      "RMSProp @ epoch 5 :: loss = 2.3041053001306206\n",
      "RMSProp @ epoch 6 :: loss = 2.3040398011075407\n",
      "RMSProp @ epoch 7 :: loss = 2.304174480407731\n",
      "RMSProp @ epoch 8 :: loss = 2.304006122322733\n",
      "RMSProp @ epoch 9 :: loss = 2.303946364663049\n",
      "RMSProp @ epoch 10 :: loss = 2.3040899788138707\n",
      "RMSProp @ epoch 11 :: loss = 2.3041130677977604\n",
      "RMSProp @ epoch 12 :: loss = 2.304160317632435\n",
      "RMSProp @ epoch 13 :: loss = 2.3042141994687793\n",
      "RMSProp @ epoch 14 :: loss = 2.304062442484695\n",
      "RMSProp @ epoch 15 :: loss = 2.3039230822500136\n",
      "RMSProp @ epoch 16 :: loss = 2.304133212642629\n",
      "RMSProp @ epoch 17 :: loss = 2.303661875379111\n",
      "RMSProp @ epoch 18 :: loss = 2.3043538733586066\n",
      "RMSProp @ epoch 19 :: loss = 2.30385009439261\n",
      "RMSProp @ epoch 20 :: loss = 2.30402466762803\n",
      "RMSProp @ epoch 21 :: loss = 2.3039864713449214\n",
      "RMSProp @ epoch 22 :: loss = 2.30404014602653\n",
      "RMSProp @ epoch 23 :: loss = 2.3043453144366297\n",
      "RMSProp @ epoch 24 :: loss = 2.3042195127971135\n",
      "RMSProp @ epoch 25 :: loss = 2.3041697377080856\n",
      "RMSProp @ epoch 26 :: loss = 2.3040441917712244\n",
      "RMSProp @ epoch 27 :: loss = 2.3040793979091685\n",
      "RMSProp @ epoch 28 :: loss = 2.3043384783303558\n",
      "RMSProp @ epoch 29 :: loss = 2.3039046350572665\n",
      "RMSProp @ epoch 30 :: loss = 2.30423725566376\n",
      "RMSProp @ epoch 31 :: loss = 2.304143983926346\n",
      "RMSProp @ epoch 32 :: loss = 2.3038620966583934\n",
      "RMSProp @ epoch 33 :: loss = 2.304037258823289\n",
      "RMSProp @ epoch 34 :: loss = 2.304219000375093\n",
      "RMSProp @ epoch 35 :: loss = 2.303985031428876\n",
      "RMSProp @ epoch 36 :: loss = 2.304065618942033\n",
      "RMSProp @ epoch 37 :: loss = 2.3041052528535886\n",
      "RMSProp @ epoch 38 :: loss = 2.3039833350476426\n",
      "RMSProp @ epoch 39 :: loss = 2.304377456209553\n",
      "RMSProp @ epoch 40 :: loss = 2.3041342240152582\n",
      "RMSProp @ epoch 41 :: loss = 2.303801082090528\n",
      "RMSProp @ epoch 42 :: loss = 2.3040761136805323\n",
      "RMSProp @ epoch 43 :: loss = 2.3039298675207696\n",
      "RMSProp @ epoch 44 :: loss = 2.3039524290861606\n",
      "RMSProp @ epoch 45 :: loss = 2.304202976765663\n",
      "RMSProp @ epoch 46 :: loss = 2.30379315607075\n",
      "RMSProp @ epoch 47 :: loss = 2.304221816916964\n",
      "RMSProp @ epoch 48 :: loss = 2.303993454874197\n",
      "RMSProp @ epoch 49 :: loss = 2.304051110485215\n",
      "RMSProp @ epoch 50 :: loss = 2.303905363021883\n",
      "RMSProp @ epoch 51 :: loss = 2.3041775554482107\n",
      "RMSProp @ epoch 52 :: loss = 2.303878078328521\n",
      "RMSProp @ epoch 53 :: loss = 2.3042041919887195\n",
      "RMSProp @ epoch 54 :: loss = 2.3039787527340563\n",
      "RMSProp @ epoch 55 :: loss = 2.3040566002127965\n",
      "RMSProp @ epoch 56 :: loss = 2.304089207130709\n",
      "RMSProp @ epoch 57 :: loss = 2.30400683554505\n",
      "RMSProp @ epoch 58 :: loss = 2.3041474920854386\n",
      "RMSProp @ epoch 59 :: loss = 2.3042196950424456\n",
      "RMSProp @ epoch 60 :: loss = 2.3039926552315\n",
      "RMSProp @ epoch 61 :: loss = 2.3041000239122145\n",
      "RMSProp @ epoch 62 :: loss = 2.30397033716824\n",
      "RMSProp @ epoch 63 :: loss = 2.3039804508945325\n",
      "RMSProp @ epoch 64 :: loss = 2.3039901215892864\n",
      "RMSProp @ epoch 65 :: loss = 2.304241163135846\n",
      "RMSProp @ epoch 66 :: loss = 2.304177956794625\n",
      "RMSProp @ epoch 67 :: loss = 2.304252905632133\n",
      "RMSProp @ epoch 68 :: loss = 2.3039303265654962\n",
      "RMSProp @ epoch 69 :: loss = 2.3042005600451407\n",
      "RMSProp @ epoch 70 :: loss = 2.3040435911495805\n"
     ]
    }
   ],
   "source": [
    "model = ClassifierNN()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.03, momentum=0.9, eps=1e-8 )\n",
    "rms_losses = train(model, optimizer, \"RMSProp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam @ epoch 1 :: loss = 0.7008632733019938\n",
      "Adam @ epoch 2 :: loss = 0.5501073752956858\n",
      "Adam @ epoch 3 :: loss = 0.577424107838287\n",
      "Adam @ epoch 4 :: loss = 0.542257477336728\n",
      "Adam @ epoch 5 :: loss = 0.5552074499348841\n",
      "Adam @ epoch 6 :: loss = 0.5593465002202022\n",
      "Adam @ epoch 7 :: loss = 0.5763628329517745\n",
      "Adam @ epoch 8 :: loss = 0.5655600128969404\n",
      "Adam @ epoch 9 :: loss = 0.5851348993906589\n",
      "Adam @ epoch 10 :: loss = 0.578043876362762\n",
      "Adam @ epoch 11 :: loss = 0.560443198280548\n",
      "Adam @ epoch 12 :: loss = 0.5546745009767984\n",
      "Adam @ epoch 13 :: loss = 0.5660284589856927\n",
      "Adam @ epoch 14 :: loss = 0.558677095514752\n",
      "Adam @ epoch 15 :: loss = 0.5659635899735412\n",
      "Adam @ epoch 16 :: loss = 0.5698063774927974\n",
      "Adam @ epoch 17 :: loss = 0.5939882439749835\n",
      "Adam @ epoch 18 :: loss = 0.6608983554533804\n",
      "Adam @ epoch 19 :: loss = 0.6551227209442205\n",
      "Adam @ epoch 20 :: loss = 0.6565487606566089\n",
      "Adam @ epoch 21 :: loss = 0.6485524595355683\n",
      "Adam @ epoch 22 :: loss = 0.6248300408106496\n",
      "Adam @ epoch 23 :: loss = 0.7066535619435026\n",
      "Adam @ epoch 24 :: loss = 0.5901632997622368\n",
      "Adam @ epoch 25 :: loss = 0.6247274413990822\n",
      "Adam @ epoch 26 :: loss = 0.6455531207991562\n",
      "Adam @ epoch 27 :: loss = 0.7251720215275343\n",
      "Adam @ epoch 28 :: loss = 0.7524762191434404\n",
      "Adam @ epoch 29 :: loss = 0.6695955391568161\n",
      "Adam @ epoch 30 :: loss = 0.7486454036348918\n",
      "Adam @ epoch 31 :: loss = 0.636604859408285\n",
      "Adam @ epoch 32 :: loss = 1.1072441695341424\n",
      "Adam @ epoch 33 :: loss = 0.8176508851206379\n",
      "Adam @ epoch 34 :: loss = 0.7171223309439129\n",
      "Adam @ epoch 35 :: loss = 0.7604345935684785\n",
      "Adam @ epoch 36 :: loss = 0.7729936759354972\n",
      "Adam @ epoch 37 :: loss = 0.7698053587029483\n",
      "Adam @ epoch 38 :: loss = 0.7719470825530826\n",
      "Adam @ epoch 39 :: loss = 0.764682560841412\n",
      "Adam @ epoch 40 :: loss = 1.3652121545727065\n",
      "Adam @ epoch 41 :: loss = 1.1678477444374231\n",
      "Adam @ epoch 42 :: loss = 1.0041117829554624\n",
      "Adam @ epoch 43 :: loss = 1.0828894901949206\n",
      "Adam @ epoch 44 :: loss = 1.2752109807945773\n",
      "Adam @ epoch 45 :: loss = 1.2663557536439347\n",
      "Adam @ epoch 46 :: loss = 1.0912231691737673\n",
      "Adam @ epoch 47 :: loss = 1.088134719745945\n",
      "Adam @ epoch 48 :: loss = 1.4095759619273611\n",
      "Adam @ epoch 49 :: loss = 1.4000815144861176\n",
      "Adam @ epoch 50 :: loss = 1.4140218591639229\n",
      "Adam @ epoch 51 :: loss = 1.2830422237864945\n",
      "Adam @ epoch 52 :: loss = 1.216910103490866\n",
      "Adam @ epoch 53 :: loss = 1.1829087651614696\n",
      "Adam @ epoch 54 :: loss = 1.1828036569456049\n",
      "Adam @ epoch 55 :: loss = 1.1735267820261688\n",
      "Adam @ epoch 56 :: loss = 1.1601961612193061\n",
      "Adam @ epoch 57 :: loss = 1.1545421328625953\n",
      "Adam @ epoch 58 :: loss = 1.1500248558866952\n",
      "Adam @ epoch 59 :: loss = 1.1531084277101162\n",
      "Adam @ epoch 60 :: loss = 1.1696255227395975\n",
      "Adam @ epoch 61 :: loss = 1.1501040382425922\n",
      "Adam @ epoch 62 :: loss = 1.1503060571293333\n",
      "Adam @ epoch 63 :: loss = 1.1516763205721434\n",
      "Adam @ epoch 64 :: loss = 1.1354240579391592\n",
      "Adam @ epoch 65 :: loss = 1.133461876464551\n",
      "Adam @ epoch 66 :: loss = 1.1345213767307907\n",
      "Adam @ epoch 67 :: loss = 1.1268158762185558\n",
      "Adam @ epoch 68 :: loss = 1.1260422878046787\n",
      "Adam @ epoch 69 :: loss = 1.1371333019565673\n",
      "Adam @ epoch 70 :: loss = 1.1372737876896157\n"
     ]
    }
   ],
   "source": [
    "model = ClassifierNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03, betas=(0.9, 0.998))\n",
    "adam_losses = train(model, optimizer, \"Adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sgd_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m6\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m plt\u001b[39m.\u001b[39mplot(sgd_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSGD\u001b[39m\u001b[39m'\u001b[39m, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39mplot(mtm_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMomentum\u001b[39m\u001b[39m'\u001b[39m, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[39m.\u001b[39mplot(nestv_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNesterov\u001b[39m\u001b[39m'\u001b[39m, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sgd_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(sgd_losses, label='SGD', color='r')\n",
    "plt.plot(mtm_losses, label='Momentum', color='g')\n",
    "plt.plot(nestv_losses, label='Nesterov', color='b')\n",
    "plt.plot(adgd_losses, label='Adagrad', color='y')\n",
    "plt.plot(rms_losses, label='RMSProp', color='m')\n",
    "plt.plot(adam_losses, label='Adam', color='k')\n",
    "\n",
    "plt.title('Loss trends')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m l \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m  \u001b[39m# this code is only for layer 3\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# the generated number that are less than 0.8 will be dropped. 80% stay, 20% dropped\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m d3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(a[l]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], a[l]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]) \u001b[39m<\u001b[39m keep_prob\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "keep_prob = 0.8   # 0 <= keep_prob <= 1\n",
    "l = 3  # this code is only for layer 3\n",
    "# the generated number that are less than 0.8 will be dropped. 80% stay, 20% dropped\n",
    "d3 = np.random.rand(a[l].shape[0], a[l].shape[1]) < keep_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
